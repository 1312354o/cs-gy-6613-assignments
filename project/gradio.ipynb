{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import hashlib\n",
    "import pymongo\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project struture:\n",
    "\n",
    "    -gradio.ipynb(gradio app)\n",
    "\n",
    "    -ETL.ipynb(Data extraction and saving)\n",
    "    \n",
    "    -Instructgen-openai.ipynb(use openai API to generate the dataset for finetuning)\n",
    "\n",
    "    -finetune.ipynb(finetuning using unsloth, runs on colab with another env)\n",
    "\n",
    "    -mongodb_data.json(raw data for mongo migration)\n",
    "\n",
    "    -docker-compose.yml docker-compose-clearml.yml docker-compose-UI.yml\n",
    "\n",
    "Github and huggingface links:\n",
    "\n",
    "    github repo:\n",
    "        \n",
    "    hugging face:\n",
    "        https://huggingface.co/1312354o/llama-tune(the model)\n",
    "\n",
    "        https://huggingface.co/datasets/1312354o/llama-ros2(dataset for fine tuning generated by gpt4o-mini)\n",
    "\n",
    "Other files:\n",
    "\n",
    "    pyproject.toml\n",
    "    \n",
    "    record.mp4(the video is on google drive:https://drive.google.com/file/d/1zO800gpaXqSyWnMJkRD6FwWxSKfpEITt/view?usp=drive_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the docker:docker-compose -f docker-compose.yml -f docker-compose-clearml.yml -f docker-compose-UI.yml up -d\n",
    "\n",
    "The running container includes rag_qdrant, ragmongo and ollama(where the model runs). Other container starts with clearml and async_delete are clearml infrastructures, pulled from clearml docker hub aka https://hub.docker.com/r/allegroai/clearml\n",
    "\n",
    "In ollama docker execute: ollama pull hf.co/1312354o/llama-tune, this is where the model runs,  I use ollama to pull the model and run it. OpenwebUI is for debugging and not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![docker page](screenshot-docker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ollama](ollma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Before starting the gradio app\n",
    "mongo_client = MongoClient(\"mongodb://localhost:27018/\")\n",
    "mongo_db = mongo_client[\"ros2_database\"]\n",
    "mongo_collection = mongo_db[\"ros2_documents\"]\n",
    "\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "qdrant_collection_name = \"ros2_vectors\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def vectorize_text(text):\n",
    "    \"\"\"vectorize text\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    embeddings = model(**tokens).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_url = \"http://10.255.255.254:11434/api/generate\" \n",
    "##api_url = \"http://localhost:11434/api/chat\"\n",
    "def generate_response(question, context, api_url=\"http://10.255.255.254:11434/api/generate\"):\n",
    "    \"\"\"\n",
    "    generate response by contacting ollama api\n",
    "    \"\"\"\n",
    "    # 准备请求数据\n",
    "    prompt = f\"Suppose you are an assistant,generate answer to the question and use the context as supplementary information：\\ncontext：{context}\\nquestion：{question}\"\n",
    "    payload = {\n",
    "        \"model\": \"hf.co/1312354o/llama-tune:latest\",  # \n",
    "        \"prompt\": prompt,\n",
    "        \"stream\":False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 发送请求\n",
    "        response = requests.post(api_url, json=payload,timeout=None)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        result = response.json()\n",
    "        return result.get(\"response\", \"no response\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"ollama fault：{e}\"\n",
    "    \n",
    "def retrive_context(question):\n",
    "    try:\n",
    "        vectorized_question = vectorize_text(question)\n",
    "        result = qdrant_client.search(collection_name=qdrant_collection_name,query_vector = vectorized_question,limit=1)\n",
    "        resultdocument = mongo_collection.find_one({\"_id\": result[0].id.replace(\"-\", \"\")})\n",
    "        return resultdocument['content']\n",
    "    except Exception:\n",
    "        return \"Context retrive error, please ignore this context when generating\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    context = retrive_context(question)\n",
    "    response = generate_response(question, context)\n",
    "    return  context, response\n",
    "\n",
    "# Gradio 前端\n",
    "\n",
    "\n",
    "preset_questions = [\n",
    "    \"Introduce ROS2\",\n",
    "    \"How to install ROS2\",\n",
    "    \"How to create a ROS2 workspace\"\n",
    "]\n",
    "\n",
    "def populate_input(selected_option, manual_input):\n",
    "    \"\"\"\n",
    "    the bar\n",
    "    \"\"\"\n",
    "    if manual_input and manual_input !=\"\":\n",
    "        return manual_input\n",
    "    return selected_option\n",
    "\n",
    "# gradio blocks\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### RAG-based QA System with Dropdown and Manual Input\")\n",
    "\n",
    "    # dropdown\n",
    "    dropdown = gr.Dropdown(choices=preset_questions, label=\"pre-defined questions\", value=None)\n",
    "    manual_input = gr.Textbox(label=\"manual input\", placeholder=\"input\")\n",
    "    \n",
    "    # submit\n",
    "    submit_btn = gr.Button(\"submit\")\n",
    "    \n",
    "    # final input\n",
    "    final_question = gr.Textbox(label=\"question\", interactive=False)\n",
    "    \n",
    "    # RAG pipeline output\n",
    "    augment_output = gr.Textbox(label=\"Context\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    # button\n",
    "    submit_btn.click(\n",
    "        populate_input,\n",
    "        inputs=[dropdown, manual_input],  \n",
    "        outputs=final_question           \n",
    "    )\n",
    "\n",
    "    # call RAG pipeline\n",
    "    final_question.change(\n",
    "        fn=rag_pipeline,\n",
    "        inputs=final_question,\n",
    "        outputs=[augment_output, answer_output]\n",
    "    )\n",
    "\n",
    "# launch\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion && potential further improvements:\n",
    "\n",
    "To be honest, my RAG system achieves little improvement compared to original llama agent. This is mainly because of data crawling pipeline requires more fine-grained data processing and data from wider source. The RAG system from the book aka LLM handbook aims at a simpler goal of answering questions while this RAG system aims at a goal of much more complexity of useful coding. And apparently we need more data and larger model to deal with this. \n",
    "\n",
    "If this system is to be improved in the future, the first and most important thing to do is to modify and extend the data crawling pipeline to get data from more sources. And we need a larger model of more parameters as the new baseline model. Given that a small model already used all of my local memory. A cloud deployment would be needed.\n",
    "\n",
    "And this is the proof of concept RAG system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
